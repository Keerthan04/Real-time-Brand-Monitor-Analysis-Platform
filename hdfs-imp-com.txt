/user/project/storage -> storage of output of spark streaming
/user/project/checkpoint -> checkpoint for spark streaming
/user/project/kafka_checkpoint ->checkpoint for kafka

pip install requests
hdfs dfs -mkdir /user/project
hdfs dfs -mkdir /user/project/storage
hdfs dfs -mkdir /user/project/checkpoint
hdfs dfs -mkdir /user/project/kafka_checkpoint

input topics of kakfa
apple_topic,samsung_topic

output topics of kafka
realtime_data

echo "export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop" >> ~/.bashrc


spark streaming running command

spark-submit \
  --conf "spark.driver.extraJavaOptions=-Dipc.maximum.data.length=67108864" \
  --conf "spark.executor.extraJavaOptions=-Dipc.maximum.data.length=67108864" \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 \
  /workdir/spark_streaming.py

spark-submit \
  --conf "spark.driver.extraJavaOptions=-Dipc.maximum.data.length=67108864" \
  --conf "spark.executor.extraJavaOptions=-Dipc.maximum.data.length=67108864" \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 \
  "C:\Users\User\OneDrive\Desktop\bda_lab_project\analysis\spark_streaming.py"

spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 \
  "C:\Users\User\OneDrive\Desktop\bda_lab_project\analysis\spark_streaming.py"

spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 \
  /workdir/spark_streaming.py

spark-submit \
  --conf "spark.hadoop.ipc.maximum.data.length=134217728" \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 \
  /workdir/spark_streaming.py


For new spark streaming file
spark-submit \
  --master local[*] \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 \
  --conf "spark.executor.memory=2g" \
  --conf "spark.driver.memory=2g" \
  /workdir/new_spark_streaming.py

hdfs dfs -mkdir -p /user/project/
hdfs dfs -mkdir -p /user/project/storage
hdfs dfs -mkdir -p /user/project/checkpoint
hdfs dfs -mkdir -p /user/project/hdfs_checkpoint
hdfs dfs -mkdir -p /user/project/kafka_checkpoint
hdfs dfs -chmod -R 777 /user/project


data
{'id': '1jezba4', 'title': "Commission provides guidance under Digital Markets Act to facilitate development of innovative products on Apple's platforms", 'score': 99, 'subreddit': 'apple', 'sentiment_class': 'Positive', 'sentiment': 1.0, 'topics': '[{"mention_count":1,"sentiment":"positive","topic":"Development of innovative products on Apple\'s platforms"}]', 'brand': 'Apple'}


Proper command to run the spark submit job and to write to hdfs also is this 
spark-submit \
  --master local[*] \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 \
  --conf "spark.executor.memory=2g" \
  --conf "spark.driver.memory=2g" \
  --conf "spark.hadoop.fs.defaultFS=hdfs://master:9000" \
  --conf "spark.hadoop.dfs.client.use.datanode.hostname=true" \
  /workdir/rate-limiting_streaming.py

#the defaultFS location and all is important to write the data

spark-submit \
  --master local[*] \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 \
  --conf "spark.executor.memory=2g" \
  --conf "spark.driver.memory=2g" \
  --conf "spark.hadoop.fs.defaultFS=hdfs://master:9000" \
  --conf "spark.hadoop.dfs.client.use.datanode.hostname=true" \
  /workdir/proper_streaming.py